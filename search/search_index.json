{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Context Management for Pydantic AI","text":"<p>Automatic conversation summarization and context management for Pydantic AI agents.</p> <p>Context Management for Pydantic AI helps your agents handle long conversations without exceeding model context limits. Choose between intelligent LLM summarization or fast sliding window trimming.</p> <ul> <li> <p> Intelligent Summarization</p> <p>LLM-powered compression that preserves key information</p> </li> <li> <p> Sliding Window</p> <p>Zero-cost message trimming for maximum speed</p> </li> <li> <p> Safe Cutoff</p> <p>Never breaks tool call/response pairs</p> </li> <li> <p> Flexible Configuration</p> <p>Message, token, or fraction-based triggers</p> </li> </ul>"},{"location":"#available-processors","title":"Available Processors","text":"Processor LLM Cost Latency Context Preservation Best For <code>SummarizationProcessor</code> High High Intelligent summary Quality-focused apps <code>SlidingWindowProcessor</code> Zero ~0ms Discards old messages Speed/cost-focused apps"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#intelligent-summarization","title":"Intelligent Summarization","text":"<p>Uses an LLM to create summaries of older messages:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    history_processors=[processor],\n)\n\nresult = await agent.run(\"Hello!\")\n</code></pre>"},{"location":"#zero-cost-sliding-window","title":"Zero-Cost Sliding Window","text":"<p>Simply discards old messages \u2014 no LLM calls:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = Agent(\n    \"openai:gpt-4o\",\n    history_processors=[processor],\n)\n\nresult = await agent.run(\"Hello!\")\n</code></pre>"},{"location":"#choosing-a-processor","title":"Choosing a Processor","text":"<p>Use SummarizationProcessor when:</p> <ul> <li>Context quality is critical</li> <li>You need to preserve key information from long conversations</li> <li>LLM cost is acceptable for your use case</li> </ul> <p>Use SlidingWindowProcessor when:</p> <ul> <li>Speed and cost are priorities</li> <li>Recent context is most important</li> <li>You're running many parallel conversations</li> <li>You want deterministic, predictable behavior</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":"Package Description Pydantic Deep Agents Full agent framework (uses this library) pydantic-ai-backend File storage and Docker sandbox pydantic-ai-todo Task planning toolset subagents-pydantic-ai Multi-agent orchestration pydantic-ai The foundation \u2014 agent framework by Pydantic"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Installation</p> <p>Get started with pip or uv</p> </li> <li> <p> Concepts</p> <p>Learn how processors work</p> </li> <li> <p> Examples</p> <p>See practical usage patterns</p> </li> <li> <p> API Reference</p> <p>Full API documentation</p> </li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#001-2025-01-20","title":"[0.0.1] - 2025-01-20","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>SummarizationProcessor - History processor that uses LLM to intelligently summarize older messages when context limits are reached</li> <li>Configurable triggers: message count, token count, or fraction of context window</li> <li>Configurable retention: keep last N messages, tokens, or fraction</li> <li>Custom token counter support</li> <li>Custom summary prompt support</li> <li> <p>Safe cutoff detection - never splits tool call/response pairs</p> </li> <li> <p>SlidingWindowProcessor - Zero-cost history processor that simply discards old messages</p> </li> <li>Same trigger and retention options as SummarizationProcessor</li> <li>No LLM calls - instant, deterministic processing</li> <li> <p>Ideal for high-throughput scenarios</p> </li> <li> <p>Factory functions for convenient processor creation:</p> </li> <li><code>create_summarization_processor()</code> - with sensible defaults</li> <li> <p><code>create_sliding_window_processor()</code> - with sensible defaults</p> </li> <li> <p>Utility functions:</p> </li> <li><code>count_tokens_approximately()</code> - heuristic token counter (~4 chars per token)</li> <li> <p><code>format_messages_for_summary()</code> - formats messages for LLM summarization</p> </li> <li> <p>Type definitions:</p> </li> <li><code>ContextSize</code> - union type for trigger/keep configuration</li> <li><code>ContextFraction</code>, <code>ContextTokens</code>, <code>ContextMessages</code> - specific context size types</li> <li> <p><code>TokenCounter</code> - callable type for custom token counters</p> </li> <li> <p>Documentation:</p> </li> <li>Full MkDocs documentation with Material theme</li> <li>Concepts, examples, and API reference</li> <li>Integration examples with pydantic-ai</li> </ul>"},{"location":"getting-help/","title":"Getting Help","text":""},{"location":"getting-help/#documentation","title":"Documentation","text":"<p>This documentation is your primary resource. Use the search bar (press <code>/</code> or <code>s</code>) to find specific topics.</p>"},{"location":"getting-help/#github-issues","title":"GitHub Issues","text":"<p>For bugs, feature requests, or questions:</p> <p> Open an Issue</p>"},{"location":"getting-help/#before-opening-an-issue","title":"Before Opening an Issue","text":"<ol> <li>Search existing issues - Your problem may already be reported</li> <li>Check the docs - The answer might be here</li> <li>Prepare a minimal example - Help us reproduce the issue</li> </ol>"},{"location":"getting-help/#bug-report-template","title":"Bug Report Template","text":"Markdown<pre><code>## Description\n[Clear description of the bug]\n\n## Steps to Reproduce\n1. Create processor with...\n2. Add to agent...\n3. Run agent...\n4. Observe error...\n\n## Expected Behavior\n[What you expected to happen]\n\n## Actual Behavior\n[What actually happened]\n\n## Environment\n- summarization-pydantic-ai version: X.X.X\n- pydantic-ai version: X.X.X\n- Python version: 3.XX\n- OS: [e.g., macOS 14.0]\n</code></pre>"},{"location":"getting-help/#community-resources","title":"Community Resources","text":""},{"location":"getting-help/#pydantic-ai","title":"Pydantic AI","text":"<p>summarization-pydantic-ai is built on Pydantic AI. Their documentation is an excellent resource:</p> <ul> <li>Pydantic AI Documentation</li> <li>Pydantic AI GitHub</li> </ul>"},{"location":"getting-help/#related-projects","title":"Related Projects","text":"<ul> <li>pydantic-deep - Full agent framework</li> <li>pydantic-ai-backend - File storage backends</li> <li>pydantic-ai-todo - Task planning toolset</li> </ul>"},{"location":"getting-help/#faq","title":"FAQ","text":""},{"location":"getting-help/#when-should-i-use-summarizationprocessor-vs-slidingwindowprocessor","title":"When should I use SummarizationProcessor vs SlidingWindowProcessor?","text":"<p>Use SummarizationProcessor when:</p> <ul> <li>Context quality matters (coding assistants, complex conversations)</li> <li>You need to preserve key decisions and information</li> <li>LLM cost is acceptable</li> </ul> <p>Use SlidingWindowProcessor when:</p> <ul> <li>Speed and cost are priorities</li> <li>Recent messages are most important</li> <li>Running many parallel conversations</li> <li>You want deterministic behavior</li> </ul>"},{"location":"getting-help/#can-i-use-this-with-models-other-than-openai","title":"Can I use this with models other than OpenAI?","text":"<p>Yes! Any model supported by Pydantic AI works:</p> Python<pre><code>from pydantic_ai_summarization import create_summarization_processor\n\n# Works with any model\nprocessor = create_summarization_processor(\n    model=\"anthropic:claude-3-5-sonnet-20241022\",\n)\n</code></pre>"},{"location":"getting-help/#how-do-i-test-without-api-calls","title":"How do I test without API calls?","text":"<p>Use <code>TestModel</code> from Pydantic AI:</p> Python<pre><code>from pydantic_ai.models.test import TestModel\nfrom pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(model=TestModel())\nagent = Agent(TestModel(), history_processors=[processor])\n</code></pre>"},{"location":"getting-help/#can-i-customize-how-summaries-are-generated","title":"Can I customize how summaries are generated?","text":"<p>Yes! Use the <code>summary_prompt</code> parameter:</p> Python<pre><code>processor = create_summarization_processor(\n    summary_prompt=\"\"\"\n    Summarize this conversation, focusing on:\n    - Key decisions made\n    - Code written or modified\n    - Outstanding questions\n\n    Conversation:\n    {messages}\n    \"\"\",\n)\n</code></pre>"},{"location":"getting-help/#how-do-i-handle-very-long-conversations","title":"How do I handle very long conversations?","text":"<p>Use multiple triggers to catch different scenarios:</p> Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4o\",\n    trigger=[\n        (\"messages\", 50),     # Many short messages\n        (\"tokens\", 100000),   # Fewer long messages\n    ],\n    keep=(\"messages\", 10),\n)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>uv (recommended) or pip</li> </ul>"},{"location":"installation/#install-with-uv-recommended","title":"Install with uv (recommended)","text":"Bash<pre><code>uv add summarization-pydantic-ai\n</code></pre>"},{"location":"installation/#install-with-pip","title":"Install with pip","text":"Bash<pre><code>pip install summarization-pydantic-ai\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"Extra Description Use Case <code>tiktoken</code> Accurate token counting OpenAI models"},{"location":"installation/#tiktoken-for-accurate-token-counting","title":"Tiktoken for Accurate Token Counting","text":"<p>For more accurate token counting (especially with OpenAI models):</p> uvpip Bash<pre><code>uv add summarization-pydantic-ai[tiktoken]\n</code></pre> Bash<pre><code>pip install summarization-pydantic-ai[tiktoken]\n</code></pre> <p>When to use tiktoken</p> <p>The default token counter uses a heuristic (~4 chars per token). For production applications with OpenAI models, tiktoken provides exact token counts.</p>"},{"location":"installation/#environment-setup","title":"Environment Setup","text":""},{"location":"installation/#api-key","title":"API Key","text":"<p>summarization-pydantic-ai uses Pydantic AI which supports multiple model providers. Set your API key:</p> OpenAIAnthropicGoogle Bash<pre><code>export OPENAI_API_KEY=your-api-key\n</code></pre> Bash<pre><code>export ANTHROPIC_API_KEY=your-api-key\n</code></pre> Bash<pre><code>export GOOGLE_API_KEY=your-api-key\n</code></pre>"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"Python<pre><code>import asyncio\nfrom pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor, __version__\n\nprint(f\"summarization-pydantic-ai version: {__version__}\")\n\nasync def main():\n    processor = create_summarization_processor(\n        trigger=(\"messages\", 50),\n        keep=(\"messages\", 10),\n    )\n\n    agent = Agent(\n        \"openai:gpt-4o\",\n        history_processors=[processor],\n    )\n\n    result = await agent.run(\"Hello!\")\n    print(f\"Agent response: {result.output}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors, ensure you have the correct Python version:</p> Bash<pre><code>python --version  # Should be 3.10+\n</code></pre>"},{"location":"installation/#api-key-not-found","title":"API Key Not Found","text":"<p>Make sure your API key is set in the environment:</p> Bash<pre><code>echo $OPENAI_API_KEY\n</code></pre>"},{"location":"installation/#pydantic-ai-not-found","title":"pydantic-ai Not Found","text":"<p>Ensure pydantic-ai is installed:</p> Bash<pre><code>pip install pydantic-ai\n# or\nuv add pydantic-ai\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts - Learn how processors work</li> <li>Basic Usage Example - Your first processor</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for summarization-pydantic-ai.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":"Python<pre><code>from pydantic_ai_summarization import (\n    # Main exports - Summarization\n    SummarizationProcessor,\n    create_summarization_processor,\n\n    # Main exports - Sliding Window\n    SlidingWindowProcessor,\n    create_sliding_window_processor,\n\n    # Utilities\n    count_tokens_approximately,\n    format_messages_for_summary,\n\n    # Types\n    ContextSize,\n    ContextFraction,\n    ContextTokens,\n    ContextMessages,\n    TokenCounter,\n\n    # Constants\n    DEFAULT_SUMMARY_PROMPT,\n)\n</code></pre>"},{"location":"api/#modules","title":"Modules","text":"Module Description Processor <code>SummarizationProcessor</code> class and factory function Sliding Window <code>SlidingWindowProcessor</code> class and factory function Types Type definitions and aliases"},{"location":"api/#processors-comparison","title":"Processors Comparison","text":"Feature SummarizationProcessor SlidingWindowProcessor LLM Cost High Zero Latency High ~0ms Context Preservation Intelligent None <code>model</code> param Required Not needed <code>summary_prompt</code> Customizable Not applicable <code>trim_tokens_to_summarize</code> Supported Not applicable"},{"location":"api/#quick-examples","title":"Quick Examples","text":""},{"location":"api/#summarization","title":"Summarization","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"api/#sliding-window","title":"Sliding Window","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"api/processor/","title":"Processor API","text":""},{"location":"api/processor/#pydantic_ai_summarization.processor","title":"<code>pydantic_ai_summarization.processor</code>","text":"<p>Summarization history processor for managing conversation context.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.DEFAULT_SUMMARY_PROMPT","title":"<code>DEFAULT_SUMMARY_PROMPT = \"&lt;role&gt;\\nContext Extraction Assistant\\n&lt;/role&gt;\\n\\n&lt;primary_objective&gt;\\nExtract the most relevant context from the conversation history below.\\n&lt;/primary_objective&gt;\\n\\n&lt;objective_information&gt;\\nYou're nearing the token limit and must extract key information. This context will overwrite the conversation history, so include only the most important information.\\n&lt;/objective_information&gt;\\n\\n&lt;instructions&gt;\\nThe conversation history will be replaced with your extracted context. Extract and record the most important context. Focus on information relevant to the overall goal. Avoid repeating completed actions.\\n&lt;/instructions&gt;\\n\\nRead the message history carefully. Think about what is most important to preserve. Extract only essential context.\\n\\nRespond ONLY with the extracted context. No additional information.\\n\\n&lt;messages&gt;\\nMessages to summarize:\\n{messages}\\n&lt;/messages&gt;\"</code>  <code>module-attribute</code>","text":"<p>Default prompt template used for generating summaries.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor","title":"<code>SummarizationProcessor</code>  <code>dataclass</code>","text":"<p>History processor that summarizes conversation when limits are reached.</p> <p>This processor monitors message token counts and automatically summarizes older messages when a threshold is reached, preserving recent messages and maintaining context continuity.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>Model to use for generating summaries.</p> <code>trigger</code> <code>ContextSize | list[ContextSize] | None</code> <p>Threshold(s) that trigger summarization.</p> <code>keep</code> <code>ContextSize</code> <p>How much context to keep after summarization.</p> <code>token_counter</code> <code>TokenCounter</code> <p>Function to count tokens in messages.</p> <code>summary_prompt</code> <code>str</code> <p>Prompt template for generating summaries.</p> <code>max_input_tokens</code> <code>int | None</code> <p>Maximum input tokens (required for fraction-based triggers).</p> <code>trim_tokens_to_summarize</code> <code>int | None</code> <p>Maximum tokens to include when generating summary.</p> Example Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 10),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>@dataclass\nclass SummarizationProcessor:\n    \"\"\"History processor that summarizes conversation when limits are reached.\n\n    This processor monitors message token counts and automatically summarizes\n    older messages when a threshold is reached, preserving recent messages\n    and maintaining context continuity.\n\n    Attributes:\n        model: Model to use for generating summaries.\n        trigger: Threshold(s) that trigger summarization.\n        keep: How much context to keep after summarization.\n        token_counter: Function to count tokens in messages.\n        summary_prompt: Prompt template for generating summaries.\n        max_input_tokens: Maximum input tokens (required for fraction-based triggers).\n        trim_tokens_to_summarize: Maximum tokens to include when generating summary.\n\n    Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_ai_summarization import SummarizationProcessor\n\n        processor = SummarizationProcessor(\n            model=\"openai:gpt-4.1\",\n            trigger=(\"tokens\", 100000),\n            keep=(\"messages\", 10),\n        )\n\n        agent = Agent(\n            \"openai:gpt-4.1\",\n            history_processors=[processor],\n        )\n        ```\n    \"\"\"\n\n    model: str\n    \"\"\"Model to use for generating summaries.\"\"\"\n\n    trigger: ContextSize | list[ContextSize] | None = None\n    \"\"\"Threshold(s) that trigger summarization.\n\n    Examples:\n        - (\"messages\", 50) - trigger when 50+ messages\n        - (\"tokens\", 100000) - trigger when 100k+ tokens\n        - (\"fraction\", 0.8) - trigger at 80% of max tokens (requires max_input_tokens)\n    \"\"\"\n\n    keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP)\n    \"\"\"How much context to keep after summarization.\n\n    Examples:\n        - (\"messages\", 20) - keep last 20 messages\n        - (\"tokens\", 10000) - keep last 10k tokens worth\n    \"\"\"\n\n    token_counter: TokenCounter = field(default=count_tokens_approximately)\n    \"\"\"Function to count tokens in messages.\"\"\"\n\n    summary_prompt: str = DEFAULT_SUMMARY_PROMPT\n    \"\"\"Prompt template for generating summaries.\"\"\"\n\n    max_input_tokens: int | None = None\n    \"\"\"Maximum input tokens for the model (required for fraction-based triggers).\"\"\"\n\n    trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT\n    \"\"\"Maximum tokens to include when generating summary. None to skip trimming.\"\"\"\n\n    _trigger_conditions: list[ContextSize] = field(default_factory=list, init=False)\n    _summarization_agent: Agent[None, str] | None = field(default=None, init=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration and set up trigger conditions.\"\"\"\n        if self.trigger is None:\n            self._trigger_conditions = []\n        elif isinstance(self.trigger, list):\n            self._trigger_conditions = [\n                self._validate_context_size(t, \"trigger\") for t in self.trigger\n            ]\n        else:\n            self._trigger_conditions = [self._validate_context_size(self.trigger, \"trigger\")]\n\n        self.keep = self._validate_context_size(self.keep, \"keep\")\n\n        # Validate that fraction-based triggers have max_input_tokens\n        requires_max_tokens = any(t[0] == \"fraction\" for t in self._trigger_conditions)\n        if self.keep[0] == \"fraction\":\n            requires_max_tokens = True\n\n        if requires_max_tokens and self.max_input_tokens is None:\n            raise ValueError(\n                \"max_input_tokens is required when using fraction-based triggers or keep. \"\n                \"Please provide the model's maximum input token limit.\"\n            )\n\n    def _validate_context_size(self, context: ContextSize, parameter_name: str) -&gt; ContextSize:\n        \"\"\"Validate context configuration tuples.\"\"\"\n        kind, value = context\n        if kind == \"fraction\":\n            if not 0 &lt; value &lt;= 1:\n                raise ValueError(\n                    f\"Fractional {parameter_name} values must be between 0 and 1, got {value}.\"\n                )\n        elif kind in {\"tokens\", \"messages\"}:\n            if value &lt;= 0:\n                raise ValueError(\n                    f\"{parameter_name} thresholds must be greater than 0, got {value}.\"\n                )\n        else:\n            raise ValueError(f\"Unsupported context size type {kind} for {parameter_name}.\")\n        return context\n\n    def _should_summarize(self, messages: list[ModelMessage], total_tokens: int) -&gt; bool:\n        \"\"\"Determine whether summarization should run.\"\"\"\n        if not self._trigger_conditions:\n            return False\n\n        for kind, value in self._trigger_conditions:\n            if kind == \"messages\" and len(messages) &gt;= value:\n                return True\n            if kind == \"tokens\" and total_tokens &gt;= value:\n                return True\n            if kind == \"fraction\" and self.max_input_tokens:\n                threshold = int(self.max_input_tokens * value)\n                if total_tokens &gt;= threshold:\n                    return True\n        return False\n\n    def _determine_cutoff_index(self, messages: list[ModelMessage]) -&gt; int:\n        \"\"\"Choose cutoff index respecting retention configuration.\"\"\"\n        kind, value = self.keep\n\n        if kind == \"messages\":\n            return self._find_safe_cutoff(messages, int(value))\n        elif kind == \"tokens\":\n            return self._find_token_based_cutoff(messages, int(value))\n        elif kind == \"fraction\" and self.max_input_tokens:\n            target_tokens = int(self.max_input_tokens * value)\n            return self._find_token_based_cutoff(messages, target_tokens)\n\n        return self._find_safe_cutoff(messages, _DEFAULT_MESSAGES_TO_KEEP)  # pragma: no cover\n\n    def _find_token_based_cutoff(\n        self, messages: list[ModelMessage], target_token_count: int\n    ) -&gt; int:\n        \"\"\"Find cutoff index based on target token retention.\"\"\"\n        if not messages or self.token_counter(messages) &lt;= target_token_count:\n            return 0\n\n        # Binary search for the cutoff point\n        left, right = 0, len(messages)\n        cutoff_candidate = len(messages)\n\n        for _ in range(len(messages).bit_length() + 1):\n            if left &gt;= right:\n                break\n\n            mid = (left + right) // 2\n            if self.token_counter(messages[mid:]) &lt;= target_token_count:\n                cutoff_candidate = mid\n                right = mid\n            else:\n                left = mid + 1\n\n        if cutoff_candidate &gt;= len(messages):  # pragma: no cover\n            cutoff_candidate = max(0, len(messages) - 1)\n\n        # Find a safe cutoff point (not splitting tool call pairs)\n        for i in range(cutoff_candidate, -1, -1):  # pragma: no branch\n            if self._is_safe_cutoff_point(messages, i):\n                return i\n\n        return 0  # pragma: no cover\n\n    def _find_safe_cutoff(self, messages: list[ModelMessage], messages_to_keep: int) -&gt; int:\n        \"\"\"Find safe cutoff point that preserves AI/Tool message pairs.\"\"\"\n        if len(messages) &lt;= messages_to_keep:\n            return 0\n\n        target_cutoff = len(messages) - messages_to_keep\n\n        for i in range(target_cutoff, -1, -1):\n            if self._is_safe_cutoff_point(messages, i):\n                return i\n\n        return 0  # pragma: no cover\n\n    def _is_safe_cutoff_point(self, messages: list[ModelMessage], cutoff_index: int) -&gt; bool:\n        \"\"\"Check if cutting at index would separate AI/Tool message pairs.\"\"\"\n        if cutoff_index &gt;= len(messages):\n            return True\n\n        search_start = max(0, cutoff_index - _SEARCH_RANGE_FOR_TOOL_PAIRS)\n        search_end = min(len(messages), cutoff_index + _SEARCH_RANGE_FOR_TOOL_PAIRS)\n\n        for i in range(search_start, search_end):\n            msg = messages[i]\n            if not isinstance(msg, ModelResponse):\n                continue\n\n            tool_call_ids: set[str] = set()\n            for part in msg.parts:\n                if isinstance(part, ToolCallPart) and part.tool_call_id:\n                    tool_call_ids.add(part.tool_call_id)\n\n            if not tool_call_ids:\n                continue\n\n            # Check if cutoff separates this tool call from its response\n            for j in range(i + 1, len(messages)):\n                check_msg = messages[j]\n                if isinstance(check_msg, ModelRequest):\n                    for request_part in check_msg.parts:\n                        if (\n                            isinstance(request_part, ToolReturnPart)\n                            and request_part.tool_call_id in tool_call_ids\n                        ):\n                            tool_before_cutoff = i &lt; cutoff_index\n                            response_before_cutoff = j &lt; cutoff_index\n                            if tool_before_cutoff != response_before_cutoff:\n                                return False\n\n        return True\n\n    def _get_summarization_agent(self) -&gt; Agent[None, str]:  # pragma: no cover\n        \"\"\"Get or create the summarization agent.\"\"\"\n        if self._summarization_agent is None:\n            self._summarization_agent = Agent(\n                self.model,\n                instructions=(\n                    \"You are a context summarization assistant. \"\n                    \"Extract the most important information from conversations.\"\n                ),\n            )\n        return self._summarization_agent\n\n    async def _create_summary(\n        self, messages_to_summarize: list[ModelMessage]\n    ) -&gt; str:  # pragma: no cover\n        \"\"\"Generate summary for the given messages.\"\"\"\n        if not messages_to_summarize:\n            return \"No previous conversation history.\"\n\n        formatted = format_messages_for_summary(messages_to_summarize)\n\n        # Trim if needed\n        if self.trim_tokens_to_summarize and len(formatted) &gt; self.trim_tokens_to_summarize * 4:\n            formatted = formatted[-(self.trim_tokens_to_summarize * 4) :]\n\n        prompt = self.summary_prompt.format(messages=formatted)\n\n        try:\n            agent = self._get_summarization_agent()\n            result = await agent.run(prompt)\n            return result.output.strip()\n        except Exception as e:\n            return f\"Error generating summary: {e!s}\"\n\n    async def __call__(self, messages: list[ModelMessage]) -&gt; list[ModelMessage]:\n        \"\"\"Process messages and summarize if needed.\n\n        This is the main entry point called by pydantic-ai's history processor mechanism.\n\n        Args:\n            messages: Current message history.\n\n        Returns:\n            Processed message history, potentially with older messages summarized.\n        \"\"\"\n        total_tokens = self.token_counter(messages)\n\n        if not self._should_summarize(messages, total_tokens):\n            return messages\n\n        cutoff_index = self._determine_cutoff_index(messages)\n\n        if cutoff_index &lt;= 0:\n            return messages\n\n        # The following code path requires an LLM call, so is covered by integration tests\n        messages_to_summarize = messages[:cutoff_index]  # pragma: no cover\n        preserved_messages = messages[cutoff_index:]  # pragma: no cover\n\n        summary = await self._create_summary(messages_to_summarize)  # pragma: no cover\n\n        # Create a summary message\n        summary_message = ModelRequest(  # pragma: no cover\n            parts=[\n                SystemPromptPart(content=f\"Summary of previous conversation:\\n\\n{summary}\"),\n            ]\n        )\n\n        return [summary_message, *preserved_messages]  # pragma: no cover\n</code></pre>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model to use for generating summaries.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.trigger","title":"<code>trigger = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Threshold(s) that trigger summarization.</p> <p>Examples:</p> <ul> <li>(\"messages\", 50) - trigger when 50+ messages</li> <li>(\"tokens\", 100000) - trigger when 100k+ tokens</li> <li>(\"fraction\", 0.8) - trigger at 80% of max tokens (requires max_input_tokens)</li> </ul>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.keep","title":"<code>keep = ('messages', _DEFAULT_MESSAGES_TO_KEEP)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How much context to keep after summarization.</p> <p>Examples:</p> <ul> <li>(\"messages\", 20) - keep last 20 messages</li> <li>(\"tokens\", 10000) - keep last 10k tokens worth</li> </ul>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.token_counter","title":"<code>token_counter = field(default=count_tokens_approximately)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Function to count tokens in messages.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.summary_prompt","title":"<code>summary_prompt = DEFAULT_SUMMARY_PROMPT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prompt template for generating summaries.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.max_input_tokens","title":"<code>max_input_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum input tokens for the model (required for fraction-based triggers).</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.trim_tokens_to_summarize","title":"<code>trim_tokens_to_summarize = _DEFAULT_TRIM_TOKEN_LIMIT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum tokens to include when generating summary. None to skip trimming.</p>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration and set up trigger conditions.</p> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration and set up trigger conditions.\"\"\"\n    if self.trigger is None:\n        self._trigger_conditions = []\n    elif isinstance(self.trigger, list):\n        self._trigger_conditions = [\n            self._validate_context_size(t, \"trigger\") for t in self.trigger\n        ]\n    else:\n        self._trigger_conditions = [self._validate_context_size(self.trigger, \"trigger\")]\n\n    self.keep = self._validate_context_size(self.keep, \"keep\")\n\n    # Validate that fraction-based triggers have max_input_tokens\n    requires_max_tokens = any(t[0] == \"fraction\" for t in self._trigger_conditions)\n    if self.keep[0] == \"fraction\":\n        requires_max_tokens = True\n\n    if requires_max_tokens and self.max_input_tokens is None:\n        raise ValueError(\n            \"max_input_tokens is required when using fraction-based triggers or keep. \"\n            \"Please provide the model's maximum input token limit.\"\n        )\n</code></pre>"},{"location":"api/processor/#pydantic_ai_summarization.processor.SummarizationProcessor.__call__","title":"<code>__call__(messages)</code>  <code>async</code>","text":"<p>Process messages and summarize if needed.</p> <p>This is the main entry point called by pydantic-ai's history processor mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ModelMessage]</code> <p>Current message history.</p> required <p>Returns:</p> Type Description <code>list[ModelMessage]</code> <p>Processed message history, potentially with older messages summarized.</p> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>async def __call__(self, messages: list[ModelMessage]) -&gt; list[ModelMessage]:\n    \"\"\"Process messages and summarize if needed.\n\n    This is the main entry point called by pydantic-ai's history processor mechanism.\n\n    Args:\n        messages: Current message history.\n\n    Returns:\n        Processed message history, potentially with older messages summarized.\n    \"\"\"\n    total_tokens = self.token_counter(messages)\n\n    if not self._should_summarize(messages, total_tokens):\n        return messages\n\n    cutoff_index = self._determine_cutoff_index(messages)\n\n    if cutoff_index &lt;= 0:\n        return messages\n\n    # The following code path requires an LLM call, so is covered by integration tests\n    messages_to_summarize = messages[:cutoff_index]  # pragma: no cover\n    preserved_messages = messages[cutoff_index:]  # pragma: no cover\n\n    summary = await self._create_summary(messages_to_summarize)  # pragma: no cover\n\n    # Create a summary message\n    summary_message = ModelRequest(  # pragma: no cover\n        parts=[\n            SystemPromptPart(content=f\"Summary of previous conversation:\\n\\n{summary}\"),\n        ]\n    )\n\n    return [summary_message, *preserved_messages]  # pragma: no cover\n</code></pre>"},{"location":"api/processor/#pydantic_ai_summarization.processor.create_summarization_processor","title":"<code>create_summarization_processor(model='openai:gpt-4.1', trigger=('tokens', _DEFAULT_TRIGGER_TOKENS), keep=('messages', _DEFAULT_MESSAGES_TO_KEEP), max_input_tokens=None, token_counter=None, summary_prompt=None)</code>","text":"<p>Create a summarization history processor.</p> <p>This is a convenience factory function for creating SummarizationProcessor instances with sensible defaults.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model to use for generating summaries. Defaults to \"openai:gpt-4.1\".</p> <code>'openai:gpt-4.1'</code> <code>trigger</code> <code>ContextSize | list[ContextSize] | None</code> <p>When to trigger summarization. Can be: - (\"messages\", N) - trigger when N+ messages - (\"tokens\", N) - trigger when N+ tokens - (\"fraction\", F) - trigger at F fraction of max_input_tokens - List of tuples to trigger on any condition Defaults to (\"tokens\", 170000).</p> <code>('tokens', _DEFAULT_TRIGGER_TOKENS)</code> <code>keep</code> <code>ContextSize</code> <p>How much context to keep after summarization. Defaults to (\"messages\", 20).</p> <code>('messages', _DEFAULT_MESSAGES_TO_KEEP)</code> <code>max_input_tokens</code> <code>int | None</code> <p>Maximum input tokens (required for fraction-based triggers).</p> <code>None</code> <code>token_counter</code> <code>TokenCounter | None</code> <p>Custom token counting function. Defaults to approximate counter.</p> <code>None</code> <code>summary_prompt</code> <code>str | None</code> <p>Custom prompt for summarization. Defaults to built-in prompt.</p> <code>None</code> <p>Returns:</p> Type Description <code>SummarizationProcessor</code> <p>Configured SummarizationProcessor.</p> Example Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 10),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>def create_summarization_processor(\n    model: str = \"openai:gpt-4.1\",\n    trigger: ContextSize | list[ContextSize] | None = (\"tokens\", _DEFAULT_TRIGGER_TOKENS),\n    keep: ContextSize = (\"messages\", _DEFAULT_MESSAGES_TO_KEEP),\n    max_input_tokens: int | None = None,\n    token_counter: TokenCounter | None = None,\n    summary_prompt: str | None = None,\n) -&gt; SummarizationProcessor:\n    \"\"\"Create a summarization history processor.\n\n    This is a convenience factory function for creating SummarizationProcessor\n    instances with sensible defaults.\n\n    Args:\n        model: Model to use for generating summaries. Defaults to \"openai:gpt-4.1\".\n        trigger: When to trigger summarization. Can be:\n            - (\"messages\", N) - trigger when N+ messages\n            - (\"tokens\", N) - trigger when N+ tokens\n            - (\"fraction\", F) - trigger at F fraction of max_input_tokens\n            - List of tuples to trigger on any condition\n            Defaults to (\"tokens\", 170000).\n        keep: How much context to keep after summarization. Defaults to (\"messages\", 20).\n        max_input_tokens: Maximum input tokens (required for fraction-based triggers).\n        token_counter: Custom token counting function. Defaults to approximate counter.\n        summary_prompt: Custom prompt for summarization. Defaults to built-in prompt.\n\n    Returns:\n        Configured SummarizationProcessor.\n\n    Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_ai_summarization import create_summarization_processor\n\n        processor = create_summarization_processor(\n            trigger=(\"messages\", 50),\n            keep=(\"messages\", 10),\n        )\n\n        agent = Agent(\n            \"openai:gpt-4.1\",\n            history_processors=[processor],\n        )\n        ```\n    \"\"\"\n    kwargs: dict[str, Any] = {\n        \"model\": model,\n        \"trigger\": trigger,\n        \"keep\": keep,\n    }\n\n    if max_input_tokens is not None:\n        kwargs[\"max_input_tokens\"] = max_input_tokens\n\n    if token_counter is not None:\n        kwargs[\"token_counter\"] = token_counter\n\n    if summary_prompt is not None:\n        kwargs[\"summary_prompt\"] = summary_prompt\n\n    return SummarizationProcessor(**kwargs)\n</code></pre>"},{"location":"api/processor/#pydantic_ai_summarization.processor.count_tokens_approximately","title":"<code>count_tokens_approximately(messages)</code>","text":"<p>Approximate token count based on character length.</p> <p>This is a simple heuristic: ~4 characters per token on average. For production use, consider using a proper tokenizer like tiktoken.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Sequence[ModelMessage]</code> <p>Sequence of messages to count tokens for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Approximate token count.</p> Example Python<pre><code>from pydantic_ai_summarization import count_tokens_approximately\n\nmessages = [...]  # Your message history\ntoken_count = count_tokens_approximately(messages)\nprint(f\"Approximately {token_count} tokens\")\n</code></pre> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>def count_tokens_approximately(messages: Sequence[ModelMessage]) -&gt; int:  # pragma: no branch\n    \"\"\"Approximate token count based on character length.\n\n    This is a simple heuristic: ~4 characters per token on average.\n    For production use, consider using a proper tokenizer like tiktoken.\n\n    Args:\n        messages: Sequence of messages to count tokens for.\n\n    Returns:\n        Approximate token count.\n\n    Example:\n        ```python\n        from pydantic_ai_summarization import count_tokens_approximately\n\n        messages = [...]  # Your message history\n        token_count = count_tokens_approximately(messages)\n        print(f\"Approximately {token_count} tokens\")\n        ```\n    \"\"\"\n    total_chars = 0\n    for msg in messages:\n        if isinstance(msg, ModelRequest):\n            for part in msg.parts:\n                if isinstance(part, UserPromptPart):\n                    if isinstance(part.content, str):\n                        total_chars += len(part.content)\n                    else:\n                        # List of content parts\n                        for item in part.content:\n                            if isinstance(item, dict) and \"text\" in item:\n                                total_chars += len(str(item.get(\"text\", \"\")))\n                elif isinstance(part, SystemPromptPart):\n                    total_chars += len(part.content)\n                elif isinstance(part, ToolReturnPart):\n                    total_chars += len(str(part.content))\n        elif isinstance(msg, ModelResponse):\n            for response_part in msg.parts:\n                if isinstance(response_part, TextPart):\n                    total_chars += len(response_part.content)\n                elif isinstance(response_part, ToolCallPart):\n                    total_chars += len(response_part.tool_name)\n                    total_chars += len(str(response_part.args))\n\n    return total_chars // 4\n</code></pre>"},{"location":"api/processor/#pydantic_ai_summarization.processor.format_messages_for_summary","title":"<code>format_messages_for_summary(messages)</code>","text":"<p>Format messages into a readable string for summarization.</p> <p>This function converts a sequence of ModelMessage objects into a human-readable format suitable for passing to an LLM for summarization.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Sequence[ModelMessage]</code> <p>Sequence of messages to format.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string representation of the messages.</p> Example Python<pre><code>from pydantic_ai_summarization import format_messages_for_summary\n\nmessages = [...]  # Your message history\nformatted = format_messages_for_summary(messages)\nprint(formatted)\n# User: Hello\n# Assistant: Hi there!\n# Tool Call [search]: {\"query\": \"weather\"}\n# Tool [search]: Sunny, 72\u00b0F\n</code></pre> Source code in <code>src/pydantic_ai_summarization/processor.py</code> Python<pre><code>def format_messages_for_summary(messages: Sequence[ModelMessage]) -&gt; str:  # pragma: no branch\n    \"\"\"Format messages into a readable string for summarization.\n\n    This function converts a sequence of ModelMessage objects into a\n    human-readable format suitable for passing to an LLM for summarization.\n\n    Args:\n        messages: Sequence of messages to format.\n\n    Returns:\n        Formatted string representation of the messages.\n\n    Example:\n        ```python\n        from pydantic_ai_summarization import format_messages_for_summary\n\n        messages = [...]  # Your message history\n        formatted = format_messages_for_summary(messages)\n        print(formatted)\n        # User: Hello\n        # Assistant: Hi there!\n        # Tool Call [search]: {\"query\": \"weather\"}\n        # Tool [search]: Sunny, 72\u00b0F\n        ```\n    \"\"\"\n    lines: list[str] = []\n\n    for msg in messages:\n        if isinstance(msg, ModelRequest):\n            lines.extend(_format_request_parts(msg))\n        elif isinstance(msg, ModelResponse):\n            lines.extend(_format_response_parts(msg))\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/sliding-window/","title":"Sliding Window API","text":""},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window","title":"<code>pydantic_ai_summarization.sliding_window</code>","text":"<p>Sliding window history processor for managing conversation context.</p> <p>This module provides a simple, zero-cost strategy for managing context window limits by keeping only the most recent messages without LLM-based summarization.</p>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor","title":"<code>SlidingWindowProcessor</code>  <code>dataclass</code>","text":"<p>History processor that keeps only the most recent messages.</p> <p>This is the simplest and most efficient strategy for managing context limits. It has zero LLM cost and near-zero latency, making it ideal for scenarios where preserving exact conversation history is less important than performance.</p> <p>Unlike SummarizationProcessor, this processor simply discards old messages without creating a summary. This means some context may be lost, but the operation is instantaneous and free.</p> <p>Attributes:</p> Name Type Description <code>trigger</code> <code>ContextSize | list[ContextSize] | None</code> <p>Threshold(s) that trigger window trimming.</p> <code>keep</code> <code>ContextSize</code> <p>How many messages to keep after trimming.</p> <code>token_counter</code> <code>TokenCounter</code> <p>Function to count tokens (only used for token-based triggers/keep).</p> <code>max_input_tokens</code> <code>int | None</code> <p>Maximum input tokens (required for fraction-based config).</p> Example Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SlidingWindowProcessor\n\n# Keep last 50 messages, trim when reaching 100\nprocessor = SlidingWindowProcessor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre> Source code in <code>src/pydantic_ai_summarization/sliding_window.py</code> Python<pre><code>@dataclass\nclass SlidingWindowProcessor:\n    \"\"\"History processor that keeps only the most recent messages.\n\n    This is the simplest and most efficient strategy for managing context limits.\n    It has zero LLM cost and near-zero latency, making it ideal for scenarios\n    where preserving exact conversation history is less important than performance.\n\n    Unlike SummarizationProcessor, this processor simply discards old messages\n    without creating a summary. This means some context may be lost, but the\n    operation is instantaneous and free.\n\n    Attributes:\n        trigger: Threshold(s) that trigger window trimming.\n        keep: How many messages to keep after trimming.\n        token_counter: Function to count tokens (only used for token-based triggers/keep).\n        max_input_tokens: Maximum input tokens (required for fraction-based config).\n\n    Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_ai_summarization import SlidingWindowProcessor\n\n        # Keep last 50 messages, trim when reaching 100\n        processor = SlidingWindowProcessor(\n            trigger=(\"messages\", 100),\n            keep=(\"messages\", 50),\n        )\n\n        agent = Agent(\n            \"openai:gpt-4.1\",\n            history_processors=[processor],\n        )\n        ```\n    \"\"\"\n\n    trigger: ContextSize | list[ContextSize] | None = None\n    \"\"\"Threshold(s) that trigger window trimming.\n\n    Examples:\n        - (\"messages\", 100) - trigger when 100+ messages\n        - (\"tokens\", 100000) - trigger when 100k+ tokens\n        - (\"fraction\", 0.8) - trigger at 80% of max tokens (requires max_input_tokens)\n        - [(\"messages\", 100), (\"tokens\", 50000)] - trigger on either condition\n    \"\"\"\n\n    keep: ContextSize = (\"messages\", _DEFAULT_WINDOW_SIZE)\n    \"\"\"How many messages to keep after trimming.\n\n    Examples:\n        - (\"messages\", 50) - keep last 50 messages\n        - (\"tokens\", 10000) - keep last 10k tokens worth\n        - (\"fraction\", 0.3) - keep last 30% of max_input_tokens\n    \"\"\"\n\n    token_counter: TokenCounter = field(default=count_tokens_approximately)\n    \"\"\"Function to count tokens in messages. Only used for token-based triggers/keep.\"\"\"\n\n    max_input_tokens: int | None = None\n    \"\"\"Maximum input tokens for the model (required for fraction-based triggers).\"\"\"\n\n    _trigger_conditions: list[ContextSize] = field(default_factory=list, init=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration and set up trigger conditions.\"\"\"\n        if self.trigger is None:\n            self._trigger_conditions = []\n        elif isinstance(self.trigger, list):\n            self._trigger_conditions = [\n                self._validate_context_size(t, \"trigger\") for t in self.trigger\n            ]\n        else:\n            self._trigger_conditions = [self._validate_context_size(self.trigger, \"trigger\")]\n\n        self.keep = self._validate_context_size(self.keep, \"keep\")\n\n        # Validate that fraction-based config has max_input_tokens\n        requires_max_tokens = any(t[0] == \"fraction\" for t in self._trigger_conditions)\n        if self.keep[0] == \"fraction\":\n            requires_max_tokens = True\n\n        if requires_max_tokens and self.max_input_tokens is None:\n            raise ValueError(\n                \"max_input_tokens is required when using fraction-based triggers or keep. \"\n                \"Please provide the model's maximum input token limit.\"\n            )\n\n    def _validate_context_size(self, context: ContextSize, parameter_name: str) -&gt; ContextSize:\n        \"\"\"Validate context configuration tuples.\"\"\"\n        kind, value = context\n        if kind == \"fraction\":\n            if not 0 &lt; value &lt;= 1:\n                raise ValueError(\n                    f\"Fractional {parameter_name} values must be between 0 and 1, got {value}.\"\n                )\n        elif kind in {\"tokens\", \"messages\"}:\n            if value &lt;= 0:\n                raise ValueError(\n                    f\"{parameter_name} thresholds must be greater than 0, got {value}.\"\n                )\n        else:\n            raise ValueError(f\"Unsupported context size type {kind} for {parameter_name}.\")\n        return context\n\n    def _should_trim(self, messages: list[ModelMessage], total_tokens: int) -&gt; bool:\n        \"\"\"Determine whether window trimming should occur.\"\"\"\n        if not self._trigger_conditions:\n            return False\n\n        for kind, value in self._trigger_conditions:\n            if kind == \"messages\" and len(messages) &gt;= value:\n                return True\n            if kind == \"tokens\" and total_tokens &gt;= value:\n                return True\n            if kind == \"fraction\" and self.max_input_tokens:\n                threshold = int(self.max_input_tokens * value)\n                if total_tokens &gt;= threshold:\n                    return True\n        return False\n\n    def _determine_cutoff_index(self, messages: list[ModelMessage]) -&gt; int:\n        \"\"\"Choose cutoff index respecting retention configuration.\"\"\"\n        kind, value = self.keep\n\n        if kind == \"messages\":\n            return self._find_safe_cutoff(messages, int(value))\n        elif kind == \"tokens\":\n            return self._find_token_based_cutoff(messages, int(value))\n        elif kind == \"fraction\" and self.max_input_tokens:\n            target_tokens = int(self.max_input_tokens * value)\n            return self._find_token_based_cutoff(messages, target_tokens)\n\n        return self._find_safe_cutoff(messages, _DEFAULT_WINDOW_SIZE)  # pragma: no cover\n\n    def _find_token_based_cutoff(\n        self, messages: list[ModelMessage], target_token_count: int\n    ) -&gt; int:\n        \"\"\"Find cutoff index based on target token retention.\"\"\"\n        if not messages or self.token_counter(messages) &lt;= target_token_count:\n            return 0\n\n        # Binary search for the cutoff point\n        left, right = 0, len(messages)\n        cutoff_candidate = len(messages)\n\n        for _ in range(len(messages).bit_length() + 1):\n            if left &gt;= right:\n                break\n\n            mid = (left + right) // 2\n            if self.token_counter(messages[mid:]) &lt;= target_token_count:\n                cutoff_candidate = mid\n                right = mid\n            else:\n                left = mid + 1\n\n        if cutoff_candidate &gt;= len(messages):  # pragma: no cover\n            cutoff_candidate = max(0, len(messages) - 1)\n\n        # Find a safe cutoff point (not splitting tool call pairs)\n        for i in range(cutoff_candidate, -1, -1):  # pragma: no branch\n            if self._is_safe_cutoff_point(messages, i):\n                return i\n\n        return 0  # pragma: no cover\n\n    def _find_safe_cutoff(self, messages: list[ModelMessage], messages_to_keep: int) -&gt; int:\n        \"\"\"Find safe cutoff point that preserves tool call/response pairs.\"\"\"\n        if len(messages) &lt;= messages_to_keep:\n            return 0\n\n        target_cutoff = len(messages) - messages_to_keep\n\n        for i in range(target_cutoff, -1, -1):\n            if self._is_safe_cutoff_point(messages, i):\n                return i\n\n        return 0  # pragma: no cover\n\n    def _is_safe_cutoff_point(self, messages: list[ModelMessage], cutoff_index: int) -&gt; bool:\n        \"\"\"Check if cutting at index would separate tool call/response pairs.\n\n        This ensures we never discard a tool call without its response or vice versa,\n        which would confuse the model.\n        \"\"\"\n        if cutoff_index &gt;= len(messages):\n            return True\n\n        search_start = max(0, cutoff_index - _SEARCH_RANGE_FOR_TOOL_PAIRS)\n        search_end = min(len(messages), cutoff_index + _SEARCH_RANGE_FOR_TOOL_PAIRS)\n\n        for i in range(search_start, search_end):\n            msg = messages[i]\n            if not isinstance(msg, ModelResponse):\n                continue\n\n            tool_call_ids: set[str] = set()\n            for part in msg.parts:\n                if isinstance(part, ToolCallPart) and part.tool_call_id:\n                    tool_call_ids.add(part.tool_call_id)\n\n            if not tool_call_ids:\n                continue\n\n            # Check if cutoff separates this tool call from its response\n            for j in range(i + 1, len(messages)):\n                check_msg = messages[j]\n                if isinstance(check_msg, ModelRequest):\n                    for request_part in check_msg.parts:\n                        if (\n                            isinstance(request_part, ToolReturnPart)\n                            and request_part.tool_call_id in tool_call_ids\n                        ):\n                            tool_before_cutoff = i &lt; cutoff_index\n                            response_before_cutoff = j &lt; cutoff_index\n                            if tool_before_cutoff != response_before_cutoff:\n                                return False\n\n        return True\n\n    async def __call__(self, messages: list[ModelMessage]) -&gt; list[ModelMessage]:\n        \"\"\"Process messages and trim if needed.\n\n        This is the main entry point called by pydantic-ai's history processor mechanism.\n\n        Args:\n            messages: Current message history.\n\n        Returns:\n            Trimmed message history if threshold was reached, otherwise unchanged.\n        \"\"\"\n        total_tokens = self.token_counter(messages)\n\n        if not self._should_trim(messages, total_tokens):\n            return messages\n\n        cutoff_index = self._determine_cutoff_index(messages)\n\n        if cutoff_index &lt;= 0:\n            return messages\n\n        # Simply discard old messages and keep recent ones\n        return messages[cutoff_index:]\n</code></pre>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.trigger","title":"<code>trigger = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Threshold(s) that trigger window trimming.</p> <p>Examples:</p> <ul> <li>(\"messages\", 100) - trigger when 100+ messages</li> <li>(\"tokens\", 100000) - trigger when 100k+ tokens</li> <li>(\"fraction\", 0.8) - trigger at 80% of max tokens (requires max_input_tokens)</li> <li>[(\"messages\", 100), (\"tokens\", 50000)] - trigger on either condition</li> </ul>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.keep","title":"<code>keep = ('messages', _DEFAULT_WINDOW_SIZE)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>How many messages to keep after trimming.</p> <p>Examples:</p> <ul> <li>(\"messages\", 50) - keep last 50 messages</li> <li>(\"tokens\", 10000) - keep last 10k tokens worth</li> <li>(\"fraction\", 0.3) - keep last 30% of max_input_tokens</li> </ul>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.token_counter","title":"<code>token_counter = field(default=count_tokens_approximately)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Function to count tokens in messages. Only used for token-based triggers/keep.</p>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.max_input_tokens","title":"<code>max_input_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum input tokens for the model (required for fraction-based triggers).</p>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration and set up trigger conditions.</p> Source code in <code>src/pydantic_ai_summarization/sliding_window.py</code> Python<pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration and set up trigger conditions.\"\"\"\n    if self.trigger is None:\n        self._trigger_conditions = []\n    elif isinstance(self.trigger, list):\n        self._trigger_conditions = [\n            self._validate_context_size(t, \"trigger\") for t in self.trigger\n        ]\n    else:\n        self._trigger_conditions = [self._validate_context_size(self.trigger, \"trigger\")]\n\n    self.keep = self._validate_context_size(self.keep, \"keep\")\n\n    # Validate that fraction-based config has max_input_tokens\n    requires_max_tokens = any(t[0] == \"fraction\" for t in self._trigger_conditions)\n    if self.keep[0] == \"fraction\":\n        requires_max_tokens = True\n\n    if requires_max_tokens and self.max_input_tokens is None:\n        raise ValueError(\n            \"max_input_tokens is required when using fraction-based triggers or keep. \"\n            \"Please provide the model's maximum input token limit.\"\n        )\n</code></pre>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.SlidingWindowProcessor.__call__","title":"<code>__call__(messages)</code>  <code>async</code>","text":"<p>Process messages and trim if needed.</p> <p>This is the main entry point called by pydantic-ai's history processor mechanism.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[ModelMessage]</code> <p>Current message history.</p> required <p>Returns:</p> Type Description <code>list[ModelMessage]</code> <p>Trimmed message history if threshold was reached, otherwise unchanged.</p> Source code in <code>src/pydantic_ai_summarization/sliding_window.py</code> Python<pre><code>async def __call__(self, messages: list[ModelMessage]) -&gt; list[ModelMessage]:\n    \"\"\"Process messages and trim if needed.\n\n    This is the main entry point called by pydantic-ai's history processor mechanism.\n\n    Args:\n        messages: Current message history.\n\n    Returns:\n        Trimmed message history if threshold was reached, otherwise unchanged.\n    \"\"\"\n    total_tokens = self.token_counter(messages)\n\n    if not self._should_trim(messages, total_tokens):\n        return messages\n\n    cutoff_index = self._determine_cutoff_index(messages)\n\n    if cutoff_index &lt;= 0:\n        return messages\n\n    # Simply discard old messages and keep recent ones\n    return messages[cutoff_index:]\n</code></pre>"},{"location":"api/sliding-window/#pydantic_ai_summarization.sliding_window.create_sliding_window_processor","title":"<code>create_sliding_window_processor(trigger=('messages', _DEFAULT_TRIGGER_MESSAGES), keep=('messages', _DEFAULT_WINDOW_SIZE), max_input_tokens=None, token_counter=None)</code>","text":"<p>Create a sliding window history processor.</p> <p>This is a convenience factory function for creating SlidingWindowProcessor instances with sensible defaults.</p> <p>Parameters:</p> Name Type Description Default <code>trigger</code> <code>ContextSize | list[ContextSize] | None</code> <p>When to trigger window trimming. Can be: - (\"messages\", N) - trigger when N+ messages - (\"tokens\", N) - trigger when N+ tokens - (\"fraction\", F) - trigger at F fraction of max_input_tokens - List of tuples to trigger on any condition Defaults to (\"messages\", 100).</p> <code>('messages', _DEFAULT_TRIGGER_MESSAGES)</code> <code>keep</code> <code>ContextSize</code> <p>How many messages to keep after trimming. Defaults to (\"messages\", 50).</p> <code>('messages', _DEFAULT_WINDOW_SIZE)</code> <code>max_input_tokens</code> <code>int | None</code> <p>Maximum input tokens (required for fraction-based triggers).</p> <code>None</code> <code>token_counter</code> <code>TokenCounter | None</code> <p>Custom token counting function. Defaults to approximate counter.</p> <code>None</code> <p>Returns:</p> Type Description <code>SlidingWindowProcessor</code> <p>Configured SlidingWindowProcessor.</p> Example Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\n# Simple: keep last 30 messages when reaching 60\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 60),\n    keep=(\"messages\", 30),\n)\n\n# Token-based: keep ~50k tokens when reaching 100k\nprocessor = create_sliding_window_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"tokens\", 50000),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre> Source code in <code>src/pydantic_ai_summarization/sliding_window.py</code> Python<pre><code>def create_sliding_window_processor(\n    trigger: ContextSize | list[ContextSize] | None = (\"messages\", _DEFAULT_TRIGGER_MESSAGES),\n    keep: ContextSize = (\"messages\", _DEFAULT_WINDOW_SIZE),\n    max_input_tokens: int | None = None,\n    token_counter: TokenCounter | None = None,\n) -&gt; SlidingWindowProcessor:\n    \"\"\"Create a sliding window history processor.\n\n    This is a convenience factory function for creating SlidingWindowProcessor\n    instances with sensible defaults.\n\n    Args:\n        trigger: When to trigger window trimming. Can be:\n            - (\"messages\", N) - trigger when N+ messages\n            - (\"tokens\", N) - trigger when N+ tokens\n            - (\"fraction\", F) - trigger at F fraction of max_input_tokens\n            - List of tuples to trigger on any condition\n            Defaults to (\"messages\", 100).\n        keep: How many messages to keep after trimming. Defaults to (\"messages\", 50).\n        max_input_tokens: Maximum input tokens (required for fraction-based triggers).\n        token_counter: Custom token counting function. Defaults to approximate counter.\n\n    Returns:\n        Configured SlidingWindowProcessor.\n\n    Example:\n        ```python\n        from pydantic_ai import Agent\n        from pydantic_ai_summarization import create_sliding_window_processor\n\n        # Simple: keep last 30 messages when reaching 60\n        processor = create_sliding_window_processor(\n            trigger=(\"messages\", 60),\n            keep=(\"messages\", 30),\n        )\n\n        # Token-based: keep ~50k tokens when reaching 100k\n        processor = create_sliding_window_processor(\n            trigger=(\"tokens\", 100000),\n            keep=(\"tokens\", 50000),\n        )\n\n        agent = Agent(\n            \"openai:gpt-4.1\",\n            history_processors=[processor],\n        )\n        ```\n    \"\"\"\n    kwargs: dict[str, ContextSize | list[ContextSize] | int | TokenCounter | None] = {\n        \"trigger\": trigger,\n        \"keep\": keep,\n    }\n\n    if max_input_tokens is not None:\n        kwargs[\"max_input_tokens\"] = max_input_tokens\n\n    if token_counter is not None:\n        kwargs[\"token_counter\"] = token_counter\n\n    return SlidingWindowProcessor(**kwargs)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/types/","title":"Types API","text":""},{"location":"api/types/#pydantic_ai_summarization.types","title":"<code>pydantic_ai_summarization.types</code>","text":"<p>Type definitions for summarization-pydantic-ai.</p>"},{"location":"api/types/#pydantic_ai_summarization.types.TokenCounter","title":"<code>TokenCounter = Callable[[Sequence[ModelMessage]], int]</code>  <code>module-attribute</code>","text":"<p>Function type that counts tokens in a sequence of messages.</p> Example Python<pre><code>def my_token_counter(messages: Sequence[ModelMessage]) -&gt; int:\n    return sum(len(str(msg)) for msg in messages) // 4\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    token_counter=my_token_counter,\n)\n</code></pre>"},{"location":"api/types/#pydantic_ai_summarization.types.ContextSize","title":"<code>ContextSize = ContextFraction | ContextTokens | ContextMessages</code>  <code>module-attribute</code>","text":"<p>Union type for all context size specifications.</p> <p>Can be: - <code>(\"fraction\", float)</code> - fraction of max_input_tokens (requires max_input_tokens) - <code>(\"tokens\", int)</code> - absolute token count - <code>(\"messages\", int)</code> - message count</p> <p>Examples:</p> Python<pre><code># Trigger at 80% of context window\ntrigger: ContextSize = (\"fraction\", 0.8)\n\n# Trigger at 100k tokens\ntrigger: ContextSize = (\"tokens\", 100000)\n\n# Trigger at 50 messages\ntrigger: ContextSize = (\"messages\", 50)\n</code></pre>"},{"location":"api/types/#pydantic_ai_summarization.types.ContextFraction","title":"<code>ContextFraction = tuple[Literal['fraction'], float]</code>  <code>module-attribute</code>","text":"<p>Context size specified as a fraction of max_input_tokens.</p> <p>Example: (\"fraction\", 0.8) means 80% of max_input_tokens.</p>"},{"location":"api/types/#pydantic_ai_summarization.types.ContextTokens","title":"<code>ContextTokens = tuple[Literal['tokens'], int]</code>  <code>module-attribute</code>","text":"<p>Context size specified as an absolute token count.</p> <p>Example: (\"tokens\", 100000) means 100,000 tokens.</p>"},{"location":"api/types/#pydantic_ai_summarization.types.ContextMessages","title":"<code>ContextMessages = tuple[Literal['messages'], int]</code>  <code>module-attribute</code>","text":"<p>Context size specified as a message count.</p> <p>Example: (\"messages\", 50) means 50 messages.</p>"},{"location":"concepts/","title":"Core Concepts","text":"<p>summarization-pydantic-ai provides automatic conversation context management for pydantic-ai agents.</p>"},{"location":"concepts/#overview","title":"Overview","text":"<p>When agent conversations grow long, they can exceed the model's context window. This library solves that by providing two strategies:</p> Strategy Description Cost Speed SummarizationProcessor Uses LLM to summarize old messages High Slow SlidingWindowProcessor Simply discards old messages Zero Instant <p>Both processors:</p> <ol> <li>Monitor conversation length (messages or tokens)</li> <li>Trigger processing when thresholds are reached</li> <li>Find safe cutoff that preserves tool call pairs</li> <li>Process older messages (summarize or discard)</li> <li>Preserve recent messages for context continuity</li> </ol>"},{"location":"concepts/#key-components","title":"Key Components","text":""},{"location":"concepts/#summarizationprocessor","title":"SummarizationProcessor","text":"<p>Intelligent summarization using an LLM:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"concepts/#slidingwindowprocessor","title":"SlidingWindowProcessor","text":"<p>Zero-cost trimming without LLM:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"concepts/#factory-functions","title":"Factory Functions","text":"<p>Convenience functions with sensible defaults:</p> Python<pre><code>from pydantic_ai_summarization import (\n    create_summarization_processor,\n    create_sliding_window_processor,\n)\n\n# Summarization with defaults\nsummarizer = create_summarization_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 10),\n)\n\n# Sliding window with defaults\nwindow = create_sliding_window_processor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n</code></pre>"},{"location":"concepts/#how-summarizationprocessor-works","title":"How SummarizationProcessor Works","text":"Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Agent Conversation                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Message 1   \u2502  Message 2   \u2502  ...  \u2502  Message N-1  \u2502 Msg N \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 SummarizationProcessor                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1. Count tokens                                            \u2502\n\u2502  2. Check triggers (messages/tokens/fraction)               \u2502\n\u2502  3. Find safe cutoff point                                  \u2502\n\u2502  4. Generate summary via LLM                                \u2502\n\u2502  5. Replace old messages with summary                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Processed Messages                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Summary Message  \u2502  Message N-19  \u2502  ...  \u2502  Message N     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/#how-slidingwindowprocessor-works","title":"How SlidingWindowProcessor Works","text":"Text Only<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Agent Conversation                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Message 1   \u2502  Message 2   \u2502  ...  \u2502  Message N-1  \u2502 Msg N \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 SlidingWindowProcessor                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1. Count messages/tokens                                   \u2502\n\u2502  2. Check triggers                                          \u2502\n\u2502  3. Find safe cutoff point                                  \u2502\n\u2502  4. Discard old messages (no LLM call)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Processed Messages                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Message N-49  \u2502  Message N-48  \u2502  ...  \u2502  Message N        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/#tool-call-safety","title":"Tool Call Safety","text":"<p>Both processors ensure tool call/response pairs are never split:</p> Text Only<pre><code>\u274c Bad cutoff (splits pair):\n[Tool Call: search] | [Tool Result: found 5 items] [User: thanks]\n                    \u2191 cutoff here\n\n\u2705 Good cutoff (preserves pair):\n[User: find items] | [Tool Call: search] [Tool Result: found 5 items]\n                   \u2191 cutoff here\n</code></pre>"},{"location":"concepts/#choosing-a-processor","title":"Choosing a Processor","text":"Requirement Recommended Context quality is critical SummarizationProcessor Speed and cost are priorities SlidingWindowProcessor Running many parallel conversations SlidingWindowProcessor Long-running coding sessions SummarizationProcessor Simple chatbots SlidingWindowProcessor Deterministic behavior needed SlidingWindowProcessor"},{"location":"concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Triggers</li> <li>See the SummarizationProcessor details</li> <li>See the SlidingWindowProcessor details</li> <li>View Examples</li> </ul>"},{"location":"concepts/processor/","title":"SummarizationProcessor","text":"<p>The <code>SummarizationProcessor</code> is the core component that handles conversation summarization.</p>"},{"location":"concepts/processor/#basic-usage","title":"Basic Usage","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"concepts/processor/#configuration","title":"Configuration","text":""},{"location":"concepts/processor/#model","title":"Model","text":"<p>The model used to generate summaries:</p> Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",  # Any pydantic-ai supported model\n    ...\n)\n</code></pre>"},{"location":"concepts/processor/#trigger","title":"Trigger","text":"<p>When to trigger summarization. See Triggers for details:</p> Python<pre><code># Single trigger\nprocessor = SummarizationProcessor(\n    trigger=(\"tokens\", 100000),\n    ...\n)\n\n# Multiple triggers (OR logic)\nprocessor = SummarizationProcessor(\n    trigger=[\n        (\"messages\", 50),\n        (\"tokens\", 100000),\n    ],\n    ...\n)\n</code></pre>"},{"location":"concepts/processor/#keep","title":"Keep","text":"<p>How much context to retain after summarization:</p> Python<pre><code># Keep last 20 messages\nprocessor = SummarizationProcessor(\n    keep=(\"messages\", 20),\n    ...\n)\n\n# Keep last 10k tokens worth\nprocessor = SummarizationProcessor(\n    keep=(\"tokens\", 10000),\n    ...\n)\n</code></pre>"},{"location":"concepts/processor/#token-counter","title":"Token Counter","text":"<p>Custom function for counting tokens:</p> Python<pre><code>def my_counter(messages):\n    # Your logic here\n    return token_count\n\nprocessor = SummarizationProcessor(\n    token_counter=my_counter,\n    ...\n)\n</code></pre>"},{"location":"concepts/processor/#summary-prompt","title":"Summary Prompt","text":"<p>Custom prompt for generating summaries:</p> Python<pre><code>processor = SummarizationProcessor(\n    summary_prompt=\"\"\"\n    Summarize this conversation, focusing on:\n    - Key decisions made\n    - Important code changes\n    - Pending tasks\n\n    {messages}\n    \"\"\",\n    ...\n)\n</code></pre>"},{"location":"concepts/processor/#tool-call-safety","title":"Tool Call Safety","text":"<p>The processor ensures tool call/response pairs are never split:</p> Text Only<pre><code>\u274c Bad cutoff (splits pair):\n[Tool Call: search] | [Tool Result: found 5 items] [User: thanks]\n                    \u2191 cutoff here\n\n\u2705 Good cutoff (preserves pair):\n[User: find items] | [Tool Call: search] [Tool Result: found 5 items]\n                   \u2191 cutoff here\n</code></pre>"},{"location":"concepts/processor/#factory-function","title":"Factory Function","text":"<p>Use <code>create_summarization_processor()</code> for common configurations:</p> Python<pre><code>from pydantic_ai_summarization import create_summarization_processor\n\n# With defaults\nprocessor = create_summarization_processor()\n\n# With custom settings\nprocessor = create_summarization_processor(\n    model=\"openai:gpt-4\",\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 10),\n)\n</code></pre>"},{"location":"concepts/sliding-window/","title":"SlidingWindowProcessor","text":"<p>The <code>SlidingWindowProcessor</code> is a zero-cost history processor that simply discards old messages when thresholds are reached.</p>"},{"location":"concepts/sliding-window/#why-use-sliding-window","title":"Why Use Sliding Window?","text":"Advantage Description Zero LLM Cost No API calls needed for processing Instant Near-zero latency - just array slicing Deterministic Predictable, reproducible behavior Simple Easy to understand and debug"},{"location":"concepts/sliding-window/#basic-usage","title":"Basic Usage","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\n# When conversation reaches 100 messages,\n# old messages are discarded, keeping last 50\nresult = await agent.run(\"Hello!\")\n</code></pre>"},{"location":"concepts/sliding-window/#configuration","title":"Configuration","text":""},{"location":"concepts/sliding-window/#trigger","title":"Trigger","text":"<p>When to trigger window trimming. See Triggers for details:</p> Python<pre><code># Single trigger - by message count\nprocessor = SlidingWindowProcessor(\n    trigger=(\"messages\", 100),\n    ...\n)\n\n# Single trigger - by token count\nprocessor = SlidingWindowProcessor(\n    trigger=(\"tokens\", 50000),\n    ...\n)\n\n# Multiple triggers (OR logic)\nprocessor = SlidingWindowProcessor(\n    trigger=[\n        (\"messages\", 100),\n        (\"tokens\", 50000),\n    ],\n    ...\n)\n</code></pre>"},{"location":"concepts/sliding-window/#keep","title":"Keep","text":"<p>How much context to retain after trimming:</p> Python<pre><code># Keep last 50 messages\nprocessor = SlidingWindowProcessor(\n    keep=(\"messages\", 50),\n    ...\n)\n\n# Keep last 25k tokens worth\nprocessor = SlidingWindowProcessor(\n    keep=(\"tokens\", 25000),\n    ...\n)\n\n# Keep last 30% of context window\nprocessor = SlidingWindowProcessor(\n    keep=(\"fraction\", 0.3),\n    max_input_tokens=128000,\n)\n</code></pre>"},{"location":"concepts/sliding-window/#token-counter","title":"Token Counter","text":"<p>Custom function for counting tokens:</p> Python<pre><code>def my_counter(messages):\n    # Your logic here\n    return token_count\n\nprocessor = SlidingWindowProcessor(\n    token_counter=my_counter,\n    ...\n)\n</code></pre>"},{"location":"concepts/sliding-window/#tool-call-safety","title":"Tool Call Safety","text":"<p>The processor ensures tool call/response pairs are never split:</p> Text Only<pre><code>\u274c Bad cutoff (splits pair):\n[Tool Call: search] | [Tool Result: found 5 items] [User: thanks]\n                    \u2191 cutoff here\n\n\u2705 Good cutoff (preserves pair):\n[User: find items] | [Tool Call: search] [Tool Result: found 5 items]\n                   \u2191 cutoff here\n</code></pre>"},{"location":"concepts/sliding-window/#factory-function","title":"Factory Function","text":"<p>Use <code>create_sliding_window_processor()</code> for common configurations:</p> Python<pre><code>from pydantic_ai_summarization import create_sliding_window_processor\n\n# With defaults (trigger at 100 messages, keep 50)\nprocessor = create_sliding_window_processor()\n\n# With custom settings\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 60),\n    keep=(\"messages\", 30),\n)\n\n# Token-based\nprocessor = create_sliding_window_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"tokens\", 50000),\n)\n</code></pre>"},{"location":"concepts/sliding-window/#comparison-with-summarizationprocessor","title":"Comparison with SummarizationProcessor","text":"Aspect SlidingWindowProcessor SummarizationProcessor Cost Zero LLM API call Latency ~0ms Depends on model Context Loss Complete (old messages gone) Minimal (summarized) Determinism Fully deterministic LLM-dependent Complexity Simple More complex Best For Speed, cost, simplicity Context quality"},{"location":"concepts/sliding-window/#example-chat-application","title":"Example: Chat Application","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\n# Simple chatbot that keeps recent context only\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    system_prompt=\"You are a helpful assistant.\",\n    history_processors=[processor],\n)\n\nasync def chat(user_input: str, history: list):\n    result = await agent.run(user_input, message_history=history)\n    return result.output, result.all_messages()\n</code></pre>"},{"location":"concepts/sliding-window/#example-high-throughput-service","title":"Example: High-Throughput Service","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SlidingWindowProcessor\n\n# Aggressive trimming for high-throughput scenarios\nprocessor = SlidingWindowProcessor(\n    trigger=[\n        (\"messages\", 30),\n        (\"tokens\", 20000),\n    ],\n    keep=(\"messages\", 10),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1-mini\",  # Fast, cheap model\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"concepts/sliding-window/#example-token-based-window","title":"Example: Token-Based Window","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\n# Keep context within token budget\nprocessor = create_sliding_window_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"tokens\", 50000),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"concepts/sliding-window/#when-to-use","title":"When to Use","text":"<p>Use SlidingWindowProcessor when:</p> <ul> <li>Speed is critical</li> <li>You want to minimize costs</li> <li>Recent context is most important</li> <li>Running many parallel conversations</li> <li>Building simple chatbots</li> <li>Deterministic behavior is required</li> </ul> <p>Consider SummarizationProcessor instead when:</p> <ul> <li>Context quality is critical</li> <li>You need to preserve key information</li> <li>Building coding assistants or complex agents</li> <li>LLM cost is acceptable</li> </ul>"},{"location":"concepts/triggers/","title":"Triggers","text":"<p>Triggers define when summarization should occur.</p>"},{"location":"concepts/triggers/#trigger-types","title":"Trigger Types","text":""},{"location":"concepts/triggers/#message-based","title":"Message-Based","text":"<p>Trigger when message count exceeds a threshold:</p> Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"messages\", 50),  # Trigger at 50+ messages\n    ...\n)\n</code></pre>"},{"location":"concepts/triggers/#token-based","title":"Token-Based","text":"<p>Trigger when token count exceeds a threshold:</p> Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),  # Trigger at 100k+ tokens\n    ...\n)\n</code></pre>"},{"location":"concepts/triggers/#fraction-based","title":"Fraction-Based","text":"<p>Trigger at a percentage of the model's context window:</p> Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"fraction\", 0.8),    # Trigger at 80% capacity\n    max_input_tokens=128000,       # GPT-4's context window\n    ...\n)\n</code></pre> <p>Required Parameter</p> <p>Fraction-based triggers require <code>max_input_tokens</code> to be set.</p>"},{"location":"concepts/triggers/#multiple-triggers","title":"Multiple Triggers","text":"<p>Combine multiple triggers with OR logic:</p> Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=[\n        (\"messages\", 50),     # OR\n        (\"tokens\", 100000),   # OR\n        (\"fraction\", 0.8),\n    ],\n    max_input_tokens=128000,\n    ...\n)\n</code></pre>"},{"location":"concepts/triggers/#keep-configuration","title":"Keep Configuration","text":"<p>The <code>keep</code> parameter uses the same format as triggers:</p> Python<pre><code># Keep last 20 messages\nkeep=(\"messages\", 20)\n\n# Keep last 10k tokens\nkeep=(\"tokens\", 10000)\n\n# Keep last 20% of context\nkeep=(\"fraction\", 0.2)\n</code></pre>"},{"location":"concepts/triggers/#common-configurations","title":"Common Configurations","text":""},{"location":"concepts/triggers/#conservative-long-conversations","title":"Conservative (Long Conversations)","text":"Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 150000),\n    keep=(\"messages\", 30),\n)\n</code></pre>"},{"location":"concepts/triggers/#aggressive-short-context-models","title":"Aggressive (Short Context Models)","text":"Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 30000),\n    keep=(\"messages\", 10),\n)\n</code></pre>"},{"location":"concepts/triggers/#fraction-based-adaptive","title":"Fraction-Based (Adaptive)","text":"Python<pre><code>processor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"fraction\", 0.75),\n    keep=(\"fraction\", 0.25),\n    max_input_tokens=128000,\n)\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Learn by example with these practical use cases.</p>"},{"location":"examples/#overview","title":"Overview","text":"Example Description Basic Usage Getting started with both processors Advanced Custom counters, prompts, and configurations"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#minimal-summarization","title":"Minimal Summarization","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor()\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"examples/#minimal-sliding-window","title":"Minimal Sliding Window","text":"Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor()\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"examples/#with-custom-trigger","title":"With Custom Trigger","text":"SummarizationSliding Window Python<pre><code>processor = create_summarization_processor(\n    trigger=(\"messages\", 30),\n    keep=(\"messages\", 10),\n)\n</code></pre> Python<pre><code>processor = create_sliding_window_processor(\n    trigger=(\"messages\", 60),\n    keep=(\"messages\", 30),\n)\n</code></pre>"},{"location":"examples/#with-multiple-triggers","title":"With Multiple Triggers","text":"SummarizationSliding Window Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=[\n        (\"messages\", 50),\n        (\"tokens\", 100000),\n    ],\n    keep=(\"messages\", 20),\n)\n</code></pre> Python<pre><code>from pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=[\n        (\"messages\", 100),\n        (\"tokens\", 50000),\n    ],\n    keep=(\"messages\", 30),\n)\n</code></pre>"},{"location":"examples/#choosing-a-processor","title":"Choosing a Processor","text":"Scenario Recommended Context quality matters <code>SummarizationProcessor</code> Speed/cost matters <code>SlidingWindowProcessor</code> Many parallel conversations <code>SlidingWindowProcessor</code> Coding assistant <code>SummarizationProcessor</code> Simple chatbot <code>SlidingWindowProcessor</code>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>See Basic Usage for detailed examples</li> <li>See Advanced for custom configurations</li> </ul>"},{"location":"examples/advanced/","title":"Advanced Usage","text":"<p>Advanced configurations and customizations.</p>"},{"location":"examples/advanced/#custom-token-counter","title":"Custom Token Counter","text":"<p>Use your own token counting logic with either processor:</p> With tiktokenSimple counter Python<pre><code>from pydantic_ai_summarization import (\n    create_summarization_processor,\n    create_sliding_window_processor,\n)\n\ndef accurate_counter(messages):\n    \"\"\"Count tokens using tiktoken.\"\"\"\n    import tiktoken\n    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n\n    total = 0\n    for msg in messages:\n        total += len(encoding.encode(str(msg)))\n    return total\n\n# With summarization\nprocessor = create_summarization_processor(\n    token_counter=accurate_counter,\n    trigger=(\"tokens\", 100000),\n)\n\n# With sliding window\nprocessor = create_sliding_window_processor(\n    token_counter=accurate_counter,\n    trigger=(\"tokens\", 100000),\n)\n</code></pre> Python<pre><code>def simple_counter(messages):\n    \"\"\"Simple character-based estimation.\"\"\"\n    return sum(len(str(msg)) for msg in messages) // 4\n\nprocessor = create_sliding_window_processor(\n    token_counter=simple_counter,\n)\n</code></pre>"},{"location":"examples/advanced/#custom-summary-prompt","title":"Custom Summary Prompt","text":"<p>Customize how summaries are generated (SummarizationProcessor only):</p> Python<pre><code>from pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    summary_prompt=\"\"\"\n    You are summarizing an agent conversation. Extract:\n\n    1. **Key Decisions**: What was decided?\n    2. **Code Changes**: What code was written/modified?\n    3. **Pending Tasks**: What still needs to be done?\n    4. **Important Context**: What context is crucial to preserve?\n\n    Conversation to summarize:\n    {messages}\n\n    Provide a concise summary that preserves essential information.\n    \"\"\",\n)\n</code></pre>"},{"location":"examples/advanced/#multiple-triggers","title":"Multiple Triggers","text":"<p>Combine multiple trigger conditions (OR logic):</p> SummarizationSliding Window Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=[\n        (\"messages\", 50),      # Trigger at 50 messages\n        (\"tokens\", 100000),    # OR at 100k tokens\n        (\"fraction\", 0.8),     # OR at 80% capacity\n    ],\n    keep=(\"messages\", 20),\n    max_input_tokens=128000,   # Required for fraction\n)\n</code></pre> Python<pre><code>from pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=[\n        (\"messages\", 100),     # Trigger at 100 messages\n        (\"tokens\", 50000),     # OR at 50k tokens\n        (\"fraction\", 0.9),     # OR at 90% capacity\n    ],\n    keep=(\"messages\", 30),\n    max_input_tokens=128000,   # Required for fraction\n)\n</code></pre>"},{"location":"examples/advanced/#fraction-based-configuration","title":"Fraction-Based Configuration","text":"<p>Use fractions for adaptive behavior:</p> SummarizationSliding Window Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"fraction\", 0.75),  # Trigger at 75% of context\n    keep=(\"fraction\", 0.25),     # Keep last 25% of context\n    max_input_tokens=128000,\n)\n</code></pre> Python<pre><code>from pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=(\"fraction\", 0.8),   # Trigger at 80% of context\n    keep=(\"fraction\", 0.3),      # Keep last 30% of context\n    max_input_tokens=128000,\n)\n</code></pre>"},{"location":"examples/advanced/#different-models-for-summarization","title":"Different Models for Summarization","text":"<p>Use a smaller/faster model for summaries:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SummarizationProcessor\n\n# Use GPT-4o-mini for summaries (faster, cheaper)\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4o-mini\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\n# Main agent uses GPT-4\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"examples/advanced/#disable-trimming","title":"Disable Trimming","text":"<p>By default, input to summarization is trimmed. Disable this:</p> Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trim_tokens_to_summarize=None,  # No trimming\n    ...\n)\n</code></pre>"},{"location":"examples/advanced/#multiple-processors","title":"Multiple Processors","text":"<p>Chain multiple history processors:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\ndef logging_processor(messages):\n    \"\"\"Log message counts.\"\"\"\n    print(f\"Processing {len(messages)} messages\")\n    return messages\n\nwindow = create_sliding_window_processor()\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[logging_processor, window],\n)\n</code></pre>"},{"location":"examples/advanced/#token-based-keep","title":"Token-Based Keep","text":"<p>Keep messages based on token count:</p> SummarizationSliding Window Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"tokens\", 20000),  # Keep ~20k tokens worth\n)\n</code></pre> Python<pre><code>from pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"tokens\", 50000),  # Keep ~50k tokens worth\n)\n</code></pre>"},{"location":"examples/advanced/#high-throughput-configuration","title":"High-Throughput Configuration","text":"<p>For scenarios with many parallel conversations:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import SlidingWindowProcessor\n\n# Aggressive trimming for high-throughput\nprocessor = SlidingWindowProcessor(\n    trigger=[\n        (\"messages\", 30),\n        (\"tokens\", 20000),\n    ],\n    keep=(\"messages\", 10),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1-mini\",  # Fast, cheap model\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"examples/advanced/#integration-with-pydantic-deep","title":"Integration with pydantic-deep","text":"<p>Use with the full agent framework:</p> SummarizationSliding Window Python<pre><code>from pydantic_deep import create_deep_agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 20),\n)\n\nagent = create_deep_agent(\n    model=\"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre> Python<pre><code>from pydantic_deep import create_deep_agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 100),\n    keep=(\"messages\", 50),\n)\n\nagent = create_deep_agent(\n    model=\"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n</code></pre>"},{"location":"examples/advanced/#hybrid-approach","title":"Hybrid Approach","text":"<p>Use both processors for different scenarios:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import (\n    create_summarization_processor,\n    create_sliding_window_processor,\n)\n\n# For important conversations - use summarization\nimportant_processor = create_summarization_processor(\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 30),\n)\n\n# For casual conversations - use sliding window\ncasual_processor = create_sliding_window_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 20),\n)\n\n# Choose processor based on context\ndef get_agent(is_important: bool):\n    processor = important_processor if is_important else casual_processor\n    return Agent(\n        \"openai:gpt-4.1\",\n        history_processors=[processor],\n    )\n</code></pre>"},{"location":"examples/advanced/#presets-for-common-use-cases","title":"Presets for Common Use Cases","text":""},{"location":"examples/advanced/#coding-assistant","title":"Coding Assistant","text":"Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 100000),\n    keep=(\"messages\", 30),\n    summary_prompt=\"\"\"\n    Summarize this coding session. Focus on:\n    - What files were modified\n    - Key code changes made\n    - Bugs fixed or introduced\n    - Pending tasks\n\n    {messages}\n    \"\"\",\n)\n</code></pre>"},{"location":"examples/advanced/#customer-support-bot","title":"Customer Support Bot","text":"Python<pre><code>from pydantic_ai_summarization import SlidingWindowProcessor\n\nprocessor = SlidingWindowProcessor(\n    trigger=(\"messages\", 30),\n    keep=(\"messages\", 15),\n)\n</code></pre>"},{"location":"examples/advanced/#research-assistant","title":"Research Assistant","text":"Python<pre><code>from pydantic_ai_summarization import SummarizationProcessor\n\nprocessor = SummarizationProcessor(\n    model=\"openai:gpt-4.1\",\n    trigger=(\"tokens\", 80000),\n    keep=(\"tokens\", 30000),\n    summary_prompt=\"\"\"\n    Summarize this research session. Focus on:\n    - Key findings and insights\n    - Sources referenced\n    - Questions answered\n    - Open questions remaining\n\n    {messages}\n    \"\"\",\n)\n</code></pre>"},{"location":"examples/basic-usage/","title":"Basic Usage","text":"<p>Get started with conversation context management.</p>"},{"location":"examples/basic-usage/#simple-agent-with-summarization","title":"Simple Agent with Summarization","text":"<p>The simplest way to add summarization to an agent:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\n# Create processor with defaults\nprocessor = create_summarization_processor()\n\n# Create agent with processor\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\n# Use the agent normally\nasync def main():\n    result = await agent.run(\"Hello!\")\n    print(result.output)\n</code></pre>"},{"location":"examples/basic-usage/#simple-agent-with-sliding-window","title":"Simple Agent with Sliding Window","text":"<p>Zero-cost context management:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\n# Create processor with defaults\nprocessor = create_sliding_window_processor()\n\n# Create agent with processor\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\n# Use the agent normally\nasync def main():\n    result = await agent.run(\"Hello!\")\n    print(result.output)\n</code></pre>"},{"location":"examples/basic-usage/#custom-thresholds","title":"Custom Thresholds","text":"<p>Configure when processing triggers:</p> SummarizationSliding Window Python<pre><code>processor = create_summarization_processor(\n    trigger=(\"tokens\", 50000),   # Trigger at 50k tokens\n    keep=(\"messages\", 15),        # Keep last 15 messages\n)\n</code></pre> Python<pre><code>processor = create_sliding_window_processor(\n    trigger=(\"tokens\", 50000),   # Trigger at 50k tokens\n    keep=(\"messages\", 25),        # Keep last 25 messages\n)\n</code></pre>"},{"location":"examples/basic-usage/#message-based-triggers","title":"Message-Based Triggers","text":"<p>Trigger based on message count:</p> SummarizationSliding Window Python<pre><code>processor = create_summarization_processor(\n    trigger=(\"messages\", 30),    # Trigger at 30 messages\n    keep=(\"messages\", 10),       # Keep last 10 messages\n)\n</code></pre> Python<pre><code>processor = create_sliding_window_processor(\n    trigger=(\"messages\", 100),   # Trigger at 100 messages\n    keep=(\"messages\", 50),       # Keep last 50 messages\n)\n</code></pre>"},{"location":"examples/basic-usage/#conversation-loop-with-summarization","title":"Conversation Loop with Summarization","text":"<p>Use summarization with a conversation loop:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\nprocessor = create_summarization_processor(\n    trigger=(\"messages\", 20),\n    keep=(\"messages\", 5),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\nasync def chat():\n    message_history = []\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"quit\":\n            break\n\n        result = await agent.run(\n            user_input,\n            message_history=message_history,\n        )\n\n        print(f\"Assistant: {result.output}\")\n        message_history = result.all_messages()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(chat())\n</code></pre>"},{"location":"examples/basic-usage/#conversation-loop-with-sliding-window","title":"Conversation Loop with Sliding Window","text":"<p>Use sliding window for faster processing:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\nasync def chat():\n    message_history = []\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"quit\":\n            break\n\n        result = await agent.run(\n            user_input,\n            message_history=message_history,\n        )\n\n        print(f\"Assistant: {result.output}\")\n        message_history = result.all_messages()\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(chat())\n</code></pre>"},{"location":"examples/basic-usage/#with-dependencies","title":"With Dependencies","text":"<p>Use with agent dependencies:</p> SummarizationSliding Window Python<pre><code>from dataclasses import dataclass\nfrom pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_summarization_processor\n\n@dataclass\nclass MyDeps:\n    user_id: str\n\nprocessor = create_summarization_processor()\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    deps_type=MyDeps,\n    history_processors=[processor],\n)\n\nasync def main():\n    deps = MyDeps(user_id=\"user123\")\n    result = await agent.run(\"Hello!\", deps=deps)\n    print(result.output)\n</code></pre> Python<pre><code>from dataclasses import dataclass\nfrom pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\n@dataclass\nclass MyDeps:\n    user_id: str\n\nprocessor = create_sliding_window_processor()\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    deps_type=MyDeps,\n    history_processors=[processor],\n)\n\nasync def main():\n    deps = MyDeps(user_id=\"user123\")\n    result = await agent.run(\"Hello!\", deps=deps)\n    print(result.output)\n</code></pre>"},{"location":"examples/basic-usage/#with-tools","title":"With Tools","text":"<p>Both processors work seamlessly with tool-using agents:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import create_sliding_window_processor\n\nprocessor = create_sliding_window_processor(\n    trigger=(\"messages\", 50),\n    keep=(\"messages\", 20),\n)\n\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[processor],\n)\n\n@agent.tool_plain\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    return f\"Weather in {city}: Sunny, 72\u00b0F\"\n\n@agent.tool_plain\ndef search(query: str) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Found 5 results for: {query}\"\n\nasync def main():\n    result = await agent.run(\"What's the weather in New York?\")\n    print(result.output)\n</code></pre> <p>Tool Call Safety</p> <p>Both processors automatically preserve tool call/response pairs when trimming messages. A tool call will never be separated from its response.</p>"},{"location":"examples/basic-usage/#comparing-processors","title":"Comparing Processors","text":"<p>Here's a side-by-side comparison of both processors with the same configuration:</p> Python<pre><code>from pydantic_ai import Agent\nfrom pydantic_ai_summarization import (\n    create_summarization_processor,\n    create_sliding_window_processor,\n)\n\n# Summarization: Uses LLM to create intelligent summary\nsummarizer = create_summarization_processor(\n    trigger=(\"messages\", 30),\n    keep=(\"messages\", 10),\n)\n\n# Sliding Window: Simply discards old messages (no LLM cost)\nwindow = create_sliding_window_processor(\n    trigger=(\"messages\", 30),\n    keep=(\"messages\", 10),\n)\n\n# Choose based on your needs\nagent = Agent(\n    \"openai:gpt-4.1\",\n    history_processors=[summarizer],  # or [window]\n)\n</code></pre>"}]}